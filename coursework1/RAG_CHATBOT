import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer
import faiss
import torch
#Tranformers reference: PiPI Tutorials: https://pypi.org/project/transformers/

#load data
df = pd.read_csv("5-starfish.csv")
texts = df.apply(lambda row: " | ".join(row.astype(str)), axis=1).tolist()
texts = [text.lower() for text in texts]

#embed texts 
embedder = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedder.encode(texts)

#FAISS index
#HuggingFace LLM Course reference: https://huggingface.co/learn/llm-course/en/chapter5/6
dimension = embeddings.shape[1]
index = faiss.IndexFlatIP(dimension)
faiss.normalize_L2(embeddings)
index.add(embeddings)

# create local LLM using tokenizer 
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
device = "cpu" #run on local CPU
model = model.to(device)

#use FAISS similarity to get context from query 
def get_context(query, k=3):
    query_embedding = embedder.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(query_embedding)
    scores, indices = index.search(query_embedding, k)
    return "\n".join([texts[i] for i in indices[0]])

#generate answer
def generate_answer(context, question, device):
    prompt = f"Answer the question in full text based on the context below:\n\nContext: {context}\n\nQuestion: {question}"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=150)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

#chatbot loop
print("Chatbot ready! Type 'exit' to quit.")

while True:
    query = input("You: ")
    if query.lower() == "exit":
        break

    query_lower = query.lower().split()

    context = get_context(query_lower, k=3)
    answer = generate_answer(context, query, device)
    print("Chatbot: ", answer)



